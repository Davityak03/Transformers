{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mfF9fYFDZVAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a4176a-7c2d-4e8d-c96e-99629954e1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
            "[1045, 5223, 2023, 1012]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer,TFAutoModelForSequenceClassification\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model=TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "sentences=[\"I've been waiting for a HuggingFace course my whole life.\",\"I hate this.\",]\n",
        "tokens=[tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids=[tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "print(ids[0])\n",
        "print(ids[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making the length of ids equal since it should have same dimensions\n",
        "all_ids=tf.constant([[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],[1045, 5223, 2023, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "print(model(all_ids).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0ldfefNeMbt",
        "outputId": "8b1de6ae-e86e-450c-ec15-e04db15c0ebb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-2.7276225  2.8789396]\n",
            " [ 1.5444429 -1.3998365]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to find which id is being used as a padding token\n",
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si1iqgHshCON",
        "outputId": "bc6e60e2-a00d-4016-9171-43ed3e3a4ed2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask=tf.constant([[   1,    1,    1,    1,    1,    1,    1,     1,     1,    1,    1,    1,    1,    1],[   1,    1,    1,    1,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]])\n"
      ],
      "metadata": {
        "id": "mHFtYlBNj95x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=model(all_ids,attention_mask=attention_mask)"
      ],
      "metadata": {
        "id": "IEB1rtQLlHgv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_g_F8HglM8L",
        "outputId": "26e9ffbf-a286-4232-8198-94679e64f091"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-2.7276225  2.8789396]\n",
            " [ 3.9497476 -3.13574  ]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(sentences,padding=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeFUvos7lRvC",
        "outputId": "feced2f6-3d8b-481c-cdd3-3d2731a36b96"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ch8AlOH0lY0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}